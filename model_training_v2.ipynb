{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981ad10c",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.85 GiB for an array with shape (192000, 1296) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Augment samples for label -1 and 1\u001b[39;00m\n\u001b[0;32m     35\u001b[0m X_augmented, y_augmented \u001b[38;5;241m=\u001b[39m augment_features(X, y, target_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m X_augmented, y_augmented \u001b[38;5;241m=\u001b[39m \u001b[43maugment_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_augmented\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_augmented\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m X_augmented, y_augmented \u001b[38;5;241m=\u001b[39m augment_features(X_augmented, y_augmented, target_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Check the new label distribution\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 32\u001b[0m, in \u001b[0;36maugment_features\u001b[1;34m(X, y, target_label, num_samples)\u001b[0m\n\u001b[0;32m     30\u001b[0m             augmented_X\u001b[38;5;241m.\u001b[39mappend(noisy_features)\n\u001b[0;32m     31\u001b[0m             augmented_y\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented_X\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39mhstack([y, np\u001b[38;5;241m.\u001b[39marray(augmented_y)])\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\arkar\\anaconda3\\envs\\raspberryturk\\lib\\site-packages\\numpy\\core\\shape_base.py:296\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    295\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.85 GiB for an array with shape (192000, 1296) and data type float64"
     ]
    }
   ],
   "source": [
    "#data augmentation for -1 and 1 labels\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Adjust the path to the file location in your Google Drive\n",
    "data_path = 'chess_images/prepared_data/hog_svm_data.npy'\n",
    "\n",
    "# Load the prepared data\n",
    "data = np.load(data_path, allow_pickle=True).item()\n",
    "\n",
    "\n",
    "# Extract features and labels\n",
    "X = data[\"features\"]\n",
    "y = data[\"labels\"]\n",
    "\n",
    "\n",
    "def augment_features(X, y, target_label, num_samples):\n",
    "    augmented_X, augmented_y = [], []\n",
    "    for i, (features, label) in enumerate(zip(X, y)):\n",
    "        if label == target_label:\n",
    "            for _ in range(num_samples):\n",
    "                # Add small noise to features\n",
    "                noisy_features = features + np.random.normal(0, 0.01, size=features.shape)\n",
    "                augmented_X.append(noisy_features)\n",
    "                augmented_y.append(label)\n",
    "    return np.vstack([X, np.array(augmented_X)]), np.hstack([y, np.array(augmented_y)])\n",
    "\n",
    "# Augment samples for label -1 and 1\n",
    "X_augmented, y_augmented = augment_features(X, y, target_label=-1, num_samples=2)\n",
    "X_augmented, y_augmented = augment_features(X_augmented, y_augmented, target_label=1, num_samples=2)\n",
    "X_augmented, y_augmented = augment_features(X_augmented, y_augmented, target_label=0, num_samples=2)\n",
    "\n",
    "# Check the new label distribution\n",
    "print(Counter(y_augmented))\n",
    "print(X_augmented.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b82cd9a-17c9-4478-88c6-6bc2404985da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: Mean Accuracy = 0.9987, Std Dev = 0.0008\n",
      "Saved SVM model to chess_images/prepared_data/svm_model.pkl\n",
      "Random Forest: Mean Accuracy = 0.9997, Std Dev = 0.0006\n",
      "Saved Random Forest model to chess_images/prepared_data/random_forest_model.pkl\n"
     ]
    }
   ],
   "source": [
    "#K Fold and Save Model\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Load dataset\n",
    "# data = np.load(\"chess_images/prepared_data/hog_svm_data.npy\", allow_pickle=True).item()\n",
    "# X, y = data[\"features\"], data[\"labels\"]\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"SVM\": SVC(kernel=\"linear\"),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    # \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "}\n",
    "\n",
    "# Use Stratified K-Fold (better for classification tasks)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation for each model and save the best one\n",
    "best_model = None\n",
    "best_score = 0\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model,X_augmented, y_augmented, cv=kfold, scoring=\"accuracy\")\n",
    "    mean_score = scores.mean()\n",
    "    print(f\"{name}: Mean Accuracy = {mean_score:.4f}, Std Dev = {scores.std():.4f}\")\n",
    "\n",
    "    # Train model on the full dataset before saving\n",
    "    model.fit(X_augmented, y_augmented)\n",
    "    model_path = f\"chess_images/prepared_data/{name.lower().replace(' ', '_')}_model.pkl\"\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Saved {name} model to {model_path}\")\n",
    "\n",
    "    # Track the best model\n",
    "    # if mean_score > best_score:\n",
    "    #     best_score = mean_score\n",
    "    #     best_model = model\n",
    "\n",
    "# Save the best model separately\n",
    "# best_model_path = \"chess_images/prepared_data/best_model.pkl\"\n",
    "# joblib.dump(best_model, best_model_path)\n",
    "# print(f\"Best model saved to {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c810ca9-afa8-41e2-801e-7554626a6e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Load dataset\n",
    "# data = np.load(\"chess_images/prepared_data/hog_svm_data.npy\", allow_pickle=True).item()\n",
    "# X, y = data[\"features\"], data[\"labels\"]\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    # \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\"),\n",
    "    \"LightGBM\": LGBMClassifier(),\n",
    "    # \"CatBoost\": CatBoostClassifier(verbose=0),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(n_estimators=100),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100)\n",
    "}\n",
    "\n",
    "# Use Stratified K-Fold (better for classification tasks)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation for each model and save the best one\n",
    "best_model = None\n",
    "best_score = 0\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_augmented, y_augmented, cv=kfold, scoring=\"accuracy\")\n",
    "    mean_score = scores.mean()\n",
    "    print(f\"{name}: Mean Accuracy = {mean_score:.4f}, Std Dev = {scores.std():.4f}\")\n",
    "\n",
    "    # Train model on the full dataset before saving\n",
    "    model.fit(X_augmented, y_augmented)\n",
    "    model_path = f\"chess_images/prepared_data/{name.lower().replace(' ', '_')}_model.pkl\"\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Saved {name} model to {model_path}\")\n",
    "\n",
    "    # Track the best model\n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_model = model\n",
    "\n",
    "# Save the best model separately\n",
    "best_model_path = \"chess_images/prepared_data/best_model.pkl\"\n",
    "joblib.dump(best_model, best_model_path)\n",
    "print(f\"Best model saved to {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27db738a-022f-45d1-bb14-886917a6aa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated FEN: 2bwbw2/ww4wb/wwbwww1w/3bbb2/3wbb2/b6b/bbbbbbbb/8\n"
     ]
    }
   ],
   "source": [
    "#loading the model and testing input as 480*480 image and output as bw_fen file\n",
    "\n",
    "import joblib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('chess_images/prepared_data/svm_model.pkl')\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    image = cv2.resize(image, (60, 60))\n",
    "    fd, _ = hog(image, orientations=9, pixels_per_cell=(8, 8),\n",
    "                cells_per_block=(2, 2), visualize=True)\n",
    "    return fd\n",
    "\n",
    "def split_and_predict_fen(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (480, 480))\n",
    "    step = 60\n",
    "    label_mapping_inverse = {1: \"w\", -1: \"b\", 0: \"1\"}  # FEN compatible labels\n",
    "    predictions = []\n",
    "\n",
    "    for row in range(8):\n",
    "        row_data = []\n",
    "        for col in range(8):\n",
    "            square = image[row * step:(row + 1) * step, col * step:(col + 1) * step]\n",
    "            features = extract_hog_features(square)\n",
    "            prediction = model.predict([features])[0]\n",
    "            row_data.append(label_mapping_inverse[prediction])\n",
    "        # Convert row data to FEN row format\n",
    "        fen_row = ''.join(row_data)\n",
    "        # Consolidate empty squares into numbers\n",
    "        compact_fen_row = ''\n",
    "        count = 0\n",
    "        for char in fen_row:\n",
    "            if char == '1':\n",
    "                count += 1\n",
    "            else:\n",
    "                if count > 0:\n",
    "                    compact_fen_row += str(count)\n",
    "                    count = 0\n",
    "                compact_fen_row += char\n",
    "        if count > 0:\n",
    "            compact_fen_row += str(count)\n",
    "        predictions.append(compact_fen_row)\n",
    "\n",
    "    # Join rows with '/' for the final FEN\n",
    "    fen_result = '/'.join(predictions)\n",
    "    return fen_result\n",
    "\n",
    "# Example usage:\n",
    "image_path = 'chess_images/test1.jpg'\n",
    "fen_output = split_and_predict_fen(image_path)\n",
    "print(\"Generated FEN:\", fen_output)\n",
    "\n",
    "# Optionally, save the FEN to a file\n",
    "with open(\"generated_bw_fen.fen\", \"w\") as file:\n",
    "    file.write(fen_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c7e7beb6-0282-4e96-a4b4-0e9dc462aecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated FEN: 2wwww2/ww4ww/wwwwwwww/3www2/3www2/w6w/wwwwwwww/8\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('chess_images/prepared_data/hog_random_forest_model.pkl')\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    image = cv2.resize(image, (60, 60))\n",
    "    fd, _ = hog(image, orientations=9, pixels_per_cell=(8, 8),\n",
    "                cells_per_block=(2, 2), visualize=True)\n",
    "    return fd\n",
    "\n",
    "def split_and_predict_fen_with_overlay(image_path, output_path='overlay_result.jpg'):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    color_image = cv2.imread(image_path)  # Read color version for overlay\n",
    "    image = cv2.resize(image, (480, 480))\n",
    "    color_image = cv2.resize(color_image, (480, 480))\n",
    "\n",
    "    step = 60\n",
    "    label_mapping_inverse = {1: \"w\", -1: \"b\", 0: \"1\"}  # FEN compatible labels\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for row in range(8):\n",
    "        row_data = []\n",
    "        for col in range(8):\n",
    "            square = image[row * step:(row + 1) * step, col * step:(col + 1) * step]\n",
    "            features = extract_hog_features(square)\n",
    "            prediction = model.predict([features])[0]\n",
    "            label = label_mapping_inverse[prediction]\n",
    "\n",
    "            # Add to FEN processing\n",
    "            row_data.append(label)\n",
    "\n",
    "            # Draw on image overlay if there is a piece\n",
    "            if label in ['w', 'b']:\n",
    "                text_color = (255, 255, 255) if label == 'w' else (0, 0, 0)\n",
    "                cv2.putText(color_image, label, (col * step + 20, row * step + 40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Convert row data to compact FEN row\n",
    "        fen_row = ''.join(row_data)\n",
    "        compact_fen_row = ''\n",
    "        count = 0\n",
    "        for char in fen_row:\n",
    "            if char == '1':\n",
    "                count += 1\n",
    "            else:\n",
    "                if count > 0:\n",
    "                    compact_fen_row += str(count)\n",
    "                    count = 0\n",
    "                compact_fen_row += char\n",
    "        if count > 0:\n",
    "            compact_fen_row += str(count)\n",
    "\n",
    "        predictions.append(compact_fen_row)\n",
    "\n",
    "    # Final FEN string\n",
    "    fen_result = '/'.join(predictions)\n",
    "    print(\"Generated FEN:\", fen_result)\n",
    "\n",
    "    # Save the image with overlay\n",
    "    cv2.imwrite(output_path, color_image)\n",
    "\n",
    "    # Optionally, save the FEN to a file\n",
    "    # with open(\"generated_bw_fen.fen\", \"w\") as file:\n",
    "    #     file.write(fen_result)\n",
    "\n",
    "# Example usage:\n",
    "image_path = 'chess_images/test.jpg'\n",
    "split_and_predict_fen_with_overlay(image_path, output_path='chess_images/overlay_result.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2adbc4-4486-4ba2-8ab1-1f90d16897dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated FEN: 2OOOO2/OO4OO/OOOOOOOO/3OOO2/3OOO2/O6O/OOOOOOOO/8\n"
     ]
    }
   ],
   "source": [
    "# Occupy or Empty Test\n",
    "\n",
    "import joblib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('chess_images/prepared_data/tf_hog_random_forest_model.pkl')\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    image = cv2.resize(image, (60, 60))\n",
    "    fd, _ = hog(image, orientations=9, pixels_per_cell=(8, 8),\n",
    "                cells_per_block=(2, 2), visualize=True)\n",
    "    return fd\n",
    "\n",
    "def split_and_predict_fen_with_overlay(image_path, output_path='overlay_result.jpg'):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    color_image = cv2.imread(image_path)  # Read color version for overlay\n",
    "    image = cv2.resize(image, (480, 480))\n",
    "    color_image = cv2.resize(color_image, (480, 480))\n",
    "\n",
    "    step = 60\n",
    "    label_mapping_inverse = {1: \"O\", 0: \"1\"}  # FEN compatible labels\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for row in range(8):\n",
    "        row_data = []\n",
    "        for col in range(8):\n",
    "            square = image[row * step:(row + 1) * step, col * step:(col + 1) * step]\n",
    "            features = extract_hog_features(square)\n",
    "            prediction = model.predict([features])[0]\n",
    "            label = label_mapping_inverse[prediction]\n",
    "\n",
    "            # Add to FEN processing\n",
    "            row_data.append(label)\n",
    "\n",
    "            # Draw on image overlay if there is a piece\n",
    "            if label in ['O', 'b']:\n",
    "                text_color = (255, 255, 255) if label == 'w' else (0, 0, 0)\n",
    "                cv2.putText(color_image, label, (col * step + 20, row * step + 40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Convert row data to compact FEN row\n",
    "        fen_row = ''.join(row_data)\n",
    "        compact_fen_row = ''\n",
    "        count = 0\n",
    "        for char in fen_row:\n",
    "            if char == '1':\n",
    "                count += 1\n",
    "            else:\n",
    "                if count > 0:\n",
    "                    compact_fen_row += str(count)\n",
    "                    count = 0\n",
    "                compact_fen_row += char\n",
    "        if count > 0:\n",
    "            compact_fen_row += str(count)\n",
    "\n",
    "        predictions.append(compact_fen_row)\n",
    "\n",
    "    # Final FEN string\n",
    "    fen_result = '/'.join(predictions)\n",
    "    print(\"Generated FEN:\", fen_result)\n",
    "\n",
    "    # Save the image with overlay\n",
    "    cv2.imwrite(output_path, color_image)\n",
    "\n",
    "# Example usage:\n",
    "image_path = 'chess_images/test1.jpg'\n",
    "split_and_predict_fen_with_overlay(image_path, output_path='chess_images/overlay_result.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4c101a-c2b7-4567-a6dd-cc576cf6f109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated FEN: wbwwwwwb/wwbwwwww/wwwwwwww/bwwbbbww/wwwbbbww/bwbwbwbb/bbbbbbbb/bwbwbwbw\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('chess_images/prepared_data/bw_hsv_random_forest_model.pkl')\n",
    "\n",
    "def extract_hsv_features(image, bins=16):\n",
    "    \"\"\"Extracts HSV color histograms from an image.\"\"\"\n",
    "    # Convert to HSV color space\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Compute histograms for each channel (H, S, V)\n",
    "    h_hist = cv2.calcHist([hsv_image], [0], None, [bins], [0, 180])  # Hue\n",
    "    s_hist = cv2.calcHist([hsv_image], [1], None, [bins], [0, 256])  # Saturation\n",
    "    v_hist = cv2.calcHist([hsv_image], [2], None, [bins], [0, 256])  # Value\n",
    "\n",
    "    # Normalize histograms\n",
    "    h_hist = cv2.normalize(h_hist, h_hist).flatten()\n",
    "    s_hist = cv2.normalize(s_hist, s_hist).flatten()\n",
    "    v_hist = cv2.normalize(v_hist, v_hist).flatten()\n",
    "\n",
    "    # Concatenate histograms into a feature vector\n",
    "    return np.concatenate((h_hist, s_hist, v_hist))\n",
    "\n",
    "def split_and_predict_fen_with_overlay(image_path, output_path='overlay_result.jpg'):\n",
    "    image = cv2.imread(image_path)\n",
    "    color_image = image.copy()  # Keep original for overlay\n",
    "    image = cv2.resize(image, (480, 480))\n",
    "    color_image = cv2.resize(color_image, (480, 480))\n",
    "\n",
    "    step = 60\n",
    "    label_mapping_inverse = {1: \"w\", -1: \"b\", 0: \"1\"}  # FEN compatible labels\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for row in range(8):\n",
    "        row_data = []\n",
    "        for col in range(8):\n",
    "            square = image[row * step:(row + 1) * step, col * step:(col + 1) * step]\n",
    "            features = extract_hsv_features(square)\n",
    "            prediction = model.predict([features])[0]\n",
    "            label = label_mapping_inverse[prediction]\n",
    "\n",
    "            # Add to FEN processing\n",
    "            row_data.append(label)\n",
    "\n",
    "            # Draw on image overlay if there is a piece\n",
    "            if label in ['w', 'b']:\n",
    "                text_color = (255, 255, 255) if label == 'w' else (0, 0, 0)\n",
    "                cv2.putText(color_image, label, (col * step + 20, row * step + 40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Convert row data to compact FEN row\n",
    "        fen_row = ''.join(row_data)\n",
    "        compact_fen_row = ''\n",
    "        count = 0\n",
    "        for char in fen_row:\n",
    "            if char == '1':\n",
    "                count += 1\n",
    "            else:\n",
    "                if count > 0:\n",
    "                    compact_fen_row += str(count)\n",
    "                    count = 0\n",
    "                compact_fen_row += char\n",
    "        if count > 0:\n",
    "            compact_fen_row += str(count)\n",
    "\n",
    "        predictions.append(compact_fen_row)\n",
    "\n",
    "    # Final FEN string\n",
    "    fen_result = '/'.join(predictions)\n",
    "    print(\"Generated FEN:\", fen_result)\n",
    "\n",
    "    # Save the image with overlay\n",
    "    cv2.imwrite(output_path, color_image)\n",
    "\n",
    "# Example usage:\n",
    "image_path = 'chess_images/test1.jpg'\n",
    "split_and_predict_fen_with_overlay(image_path, output_path='chess_images/overlay_result2.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b01acb-4f81-4913-aedb-e0e3bcba5bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated FEN: 2wwww2/ww4ww/wwwwwwww/3bbb2/3bbb2/b6b/bbbbbbbb/8\n"
     ]
    }
   ],
   "source": [
    "# HOG and HSV combined Test\n",
    "\n",
    "import joblib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    image = cv2.resize(image, (60, 60))\n",
    "    fd, _ = hog(image, orientations=9, pixels_per_cell=(8, 8),\n",
    "                cells_per_block=(2, 2), visualize=True)\n",
    "    return fd\n",
    "\n",
    "def extract_hsv_features(image, bins=16):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    h_hist = cv2.calcHist([hsv_image], [0], None, [bins], [0, 180])\n",
    "    s_hist = cv2.calcHist([hsv_image], [1], None, [bins], [0, 256])\n",
    "    v_hist = cv2.calcHist([hsv_image], [2], None, [bins], [0, 256])\n",
    "    h_hist = cv2.normalize(h_hist, h_hist).flatten()\n",
    "    s_hist = cv2.normalize(s_hist, s_hist).flatten()\n",
    "    v_hist = cv2.normalize(v_hist, v_hist).flatten()\n",
    "    return np.concatenate((h_hist, s_hist, v_hist))\n",
    "\n",
    "def process_chessboard(image_path, output_path='chess_images/overlay_result3.jpg'):\n",
    "    hog_model = joblib.load('chess_images/prepared_data/tf_hog_random_forest_model.pkl')\n",
    "    hsv_model = joblib.load('chess_images/prepared_data/bw_hsv_random_forest_model.pkl')\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    grayscale_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    color_image = image.copy()\n",
    "    image = cv2.resize(image, (480, 480))\n",
    "    grayscale_image = cv2.resize(grayscale_image, (480, 480))\n",
    "    color_image = cv2.resize(color_image, (480, 480))\n",
    "    \n",
    "    step = 60\n",
    "    predictions = []\n",
    "    \n",
    "    for row in range(8):\n",
    "        row_data = []\n",
    "        for col in range(8):\n",
    "            square_gray = grayscale_image[row * step:(row + 1) * step, col * step:(col + 1) * step]\n",
    "            square_color = image[row * step:(row + 1) * step, col * step:(col + 1) * step]\n",
    "            \n",
    "            # Check if square is occupied\n",
    "            hog_features = extract_hog_features(square_gray)\n",
    "            is_occupied = hog_model.predict([hog_features])[0]  # 1 = occupied, 0 = empty\n",
    "            \n",
    "            if is_occupied == 0:\n",
    "                row_data.append(\"1\")\n",
    "                continue\n",
    "            \n",
    "            # Determine piece color\n",
    "            hsv_features = extract_hsv_features(square_color)\n",
    "            piece_color = hsv_model.predict([hsv_features])[0]  # 1 = white, -1 = black\n",
    "            label = 'w' if piece_color == 1 else 'b'\n",
    "            row_data.append(label)\n",
    "            \n",
    "            text_color = (255, 255, 255) if label == 'w' else (0, 0, 0)\n",
    "            cv2.putText(color_image, label, (col * step + 20, row * step + 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2, cv2.LINE_AA)\n",
    "            \n",
    "        # Convert row data to FEN format\n",
    "        fen_row = ''.join(row_data)\n",
    "        compact_fen_row = ''\n",
    "        count = 0\n",
    "        for char in fen_row:\n",
    "            if char == '1':\n",
    "                count += 1\n",
    "            else:\n",
    "                if count > 0:\n",
    "                    compact_fen_row += str(count)\n",
    "                    count = 0\n",
    "                compact_fen_row += char\n",
    "        if count > 0:\n",
    "            compact_fen_row += str(count)\n",
    "        \n",
    "        predictions.append(compact_fen_row)\n",
    "    \n",
    "    fen_result = '/'.join(predictions)\n",
    "    print(\"Generated FEN:\", fen_result)\n",
    "    \n",
    "    # Save overlay image\n",
    "    cv2.imwrite(output_path, color_image)\n",
    "    return fen_result\n",
    "\n",
    "# Example usage\n",
    "image_path = 'chess_images/test1.jpg'\n",
    "fen_string = process_chessboard(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0c606-425b-4336-acc7-d91af453cd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raspberryturk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
