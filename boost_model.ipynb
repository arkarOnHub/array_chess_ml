{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 105600, -1: 52800, 1: 52800})\n",
      "(211200, 1296)\n"
     ]
    }
   ],
   "source": [
    "#data augmentation for -1 and 1 labels\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Adjust the path to the file location in your Google Drive\n",
    "data_path = 'chess_images/prepared_data/hog_svm_data.npy'\n",
    "\n",
    "# Load the prepared data\n",
    "data = np.load(data_path, allow_pickle=True).item()\n",
    "\n",
    "\n",
    "# Extract features and labels\n",
    "X = data[\"features\"]\n",
    "y = data[\"labels\"]\n",
    "\n",
    "\n",
    "def augment_features(X, y, target_label, num_samples):\n",
    "    augmented_X, augmented_y = [], []\n",
    "    for i, (features, label) in enumerate(zip(X, y)):\n",
    "        if label == target_label:\n",
    "            for _ in range(num_samples):\n",
    "                # Add small noise to features\n",
    "                noisy_features = features + np.random.normal(0, 0.01, size=features.shape)\n",
    "                augmented_X.append(noisy_features)\n",
    "                augmented_y.append(label)\n",
    "    return np.vstack([X, np.array(augmented_X)]), np.hstack([y, np.array(augmented_y)])\n",
    "\n",
    "# Augment samples for label -1 and 1\n",
    "X_augmented, y_augmented = augment_features(X, y, target_label=-1, num_samples=2)\n",
    "X_augmented, y_augmented = augment_features(X_augmented, y_augmented, target_label=1, num_samples=2)\n",
    "X_augmented, y_augmented = augment_features(X_augmented, y_augmented, target_label=0, num_samples=2)\n",
    "\n",
    "# Check the new label distribution\n",
    "print(Counter(y_augmented))\n",
    "print(X_augmented.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arkar\\anaconda3\\envs\\raspberryturk\\lib\\site-packages\\dask\\dataframe\\__init__.py:31: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.865139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 330480\n",
      "[LightGBM] [Info] Number of data points in the train set: 168960, number of used features: 1296\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -0.693147\n",
      "[LightGBM] [Info] Start training from score -1.386294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arkar\\anaconda3\\envs\\raspberryturk\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.921471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 330480\n",
      "[LightGBM] [Info] Number of data points in the train set: 168960, number of used features: 1296\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -0.693147\n",
      "[LightGBM] [Info] Start training from score -1.386294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arkar\\anaconda3\\envs\\raspberryturk\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.942140 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 330480\n",
      "[LightGBM] [Info] Number of data points in the train set: 168960, number of used features: 1296\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -0.693147\n",
      "[LightGBM] [Info] Start training from score -1.386294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arkar\\anaconda3\\envs\\raspberryturk\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.872748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 330480\n",
      "[LightGBM] [Info] Number of data points in the train set: 168960, number of used features: 1296\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -0.693147\n",
      "[LightGBM] [Info] Start training from score -1.386294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arkar\\anaconda3\\envs\\raspberryturk\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.876293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 330480\n",
      "[LightGBM] [Info] Number of data points in the train set: 168960, number of used features: 1296\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -0.693147\n",
      "[LightGBM] [Info] Start training from score -1.386294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arkar\\anaconda3\\envs\\raspberryturk\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM: Mean Accuracy = 0.9994, Std Dev = 0.0001\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.404004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 330480\n",
      "[LightGBM] [Info] Number of data points in the train set: 211200, number of used features: 1296\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -0.693147\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "Saved LightGBM model to chess_images/prepared_data/lightgbm_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Load dataset\n",
    "# data = np.load(\"chess_images/prepared_data/hog_svm_data.npy\", allow_pickle=True).item()\n",
    "# X, y = data[\"features\"], data[\"labels\"]\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    # \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\"),\n",
    "    \"LightGBM\": LGBMClassifier(),\n",
    "    # \"CatBoost\": CatBoostClassifier(verbose=0),\n",
    "    # \"Extra Trees\": ExtraTreesClassifier(n_estimators=100),\n",
    "    # \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    # \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100)\n",
    "}\n",
    "\n",
    "# Use Stratified K-Fold (better for classification tasks)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation for each model and save the best one\n",
    "best_model = None\n",
    "best_score = 0\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_augmented, y_augmented, cv=kfold, scoring=\"accuracy\")\n",
    "    mean_score = scores.mean()\n",
    "    print(f\"{name}: Mean Accuracy = {mean_score:.4f}, Std Dev = {scores.std():.4f}\")\n",
    "\n",
    "    # Train model on the full dataset before saving\n",
    "    model.fit(X_augmented, y_augmented)\n",
    "    model_path = f\"chess_images/prepared_data/{name.lower().replace(' ', '_')}_model.pkl\"\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Saved {name} model to {model_path}\")\n",
    "\n",
    "    # Track the best model\n",
    "    # if mean_score > best_score:\n",
    "    #     best_score = mean_score\n",
    "    #     best_model = model\n",
    "\n",
    "# Save the best model separately\n",
    "# best_model_path = \"chess_images/prepared_data/best_model.pkl\"\n",
    "# joblib.dump(best_model, best_model_path)\n",
    "# print(f\"Best model saved to {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated FEN: 2bwww2/ww4ww/wwwwww1w/3bbb2/3bbw2/w6b/bbbbbbbb/8\n"
     ]
    }
   ],
   "source": [
    "#loading the model and testing input as 480*480 image and output as bw_fen file 2222222222222222\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('chess_images/prepared_data/lightgbm_model.pkl')\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    image = cv2.resize(image, (60, 60))\n",
    "    fd, _ = hog(image, orientations=9, pixels_per_cell=(8, 8),\n",
    "                cells_per_block=(2, 2), visualize=True)\n",
    "    \n",
    "    # Convert features to a DataFrame with column names matching training data\n",
    "    feature_names = [f'feature_{i}' for i in range(len(fd))]  # Create feature names\n",
    "    return pd.DataFrame([fd], columns=feature_names)  # Wrap in DataFrame\n",
    "\n",
    "def split_and_predict_fen(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (480, 480))\n",
    "    step = 60\n",
    "    label_mapping_inverse = {1: \"w\", -1: \"b\", 0: \"1\"}  # FEN compatible labels\n",
    "    predictions = []\n",
    "\n",
    "    for row in range(8):\n",
    "        row_data = []\n",
    "        for col in range(8):\n",
    "            square = image[row * step:(row + 1) * step, col * step:(col + 1) * step]\n",
    "            features_df = extract_hog_features(square)  # Get features as DataFrame\n",
    "            prediction = model.predict(features_df)[0]  # Pass DataFrame instead of list\n",
    "            row_data.append(label_mapping_inverse[prediction])\n",
    "        # Convert row data to FEN row format\n",
    "        fen_row = ''.join(row_data)\n",
    "        # Consolidate empty squares into numbers\n",
    "        compact_fen_row = ''\n",
    "        count = 0\n",
    "        for char in fen_row:\n",
    "            if char == '1':\n",
    "                count += 1\n",
    "            else:\n",
    "                if count > 0:\n",
    "                    compact_fen_row += str(count)\n",
    "                    count = 0\n",
    "                compact_fen_row += char\n",
    "        if count > 0:\n",
    "            compact_fen_row += str(count)\n",
    "        predictions.append(compact_fen_row)\n",
    "\n",
    "    # Join rows with '/' for the final FEN\n",
    "    fen_result = '/'.join(predictions)\n",
    "    return fen_result\n",
    "\n",
    "# Example usage:\n",
    "image_path = 'chess_images/test1.jpg'\n",
    "fen_output = split_and_predict_fen(image_path)\n",
    "print(\"Generated FEN:\", fen_output)\n",
    "\n",
    "# Optionally, save the FEN to a file\n",
    "with open(\"generated_bw_fen.fen\", \"w\") as file:\n",
    "    file.write(fen_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated FEN: 2wwbwwb/wwb1bww1/wwwbww1b/2w5/1b1w1b2/w2ww3/wb1wbb2/6b1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('chess_images/prepared_data/lightgbm_model.pkl')\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    image = cv2.resize(image, (60, 60))\n",
    "    fd, _ = hog(image, orientations=9, pixels_per_cell=(8, 8),\n",
    "                cells_per_block=(2, 2), visualize=True)\n",
    "    feature_names = [f'feature_{i}' for i in range(len(fd))]\n",
    "    return pd.DataFrame([fd], columns=feature_names)\n",
    "\n",
    "def split_and_predict_fen_with_overlay(image_path, output_path='chess_images/overlay_result.jpg'):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    color_image = cv2.imread(image_path)  # For overlay\n",
    "    image = cv2.resize(image, (480, 480))\n",
    "    color_image = cv2.resize(color_image, (480, 480))\n",
    "\n",
    "    step = 60\n",
    "    label_mapping_inverse = {1: \"w\", -1: \"b\", 0: \"1\"}\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for row in range(8):\n",
    "        row_data = []\n",
    "        for col in range(8):\n",
    "            square = image[row * step:(row + 1) * step, col * step:(col + 1) * step]\n",
    "            features_df = extract_hog_features(square)\n",
    "            prediction = model.predict(features_df)[0]\n",
    "            label = label_mapping_inverse[prediction]\n",
    "\n",
    "            row_data.append(label)\n",
    "\n",
    "            if label in ['w', 'b']:\n",
    "                text_color = (255, 255, 255) if label == 'w' else (0, 0, 0)\n",
    "                cv2.putText(color_image, label, (col * step + 20, row * step + 40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        fen_row = ''.join(row_data)\n",
    "        compact_fen_row = ''\n",
    "        count = 0\n",
    "        for char in fen_row:\n",
    "            if char == '1':\n",
    "                count += 1\n",
    "            else:\n",
    "                if count > 0:\n",
    "                    compact_fen_row += str(count)\n",
    "                    count = 0\n",
    "                compact_fen_row += char\n",
    "        if count > 0:\n",
    "            compact_fen_row += str(count)\n",
    "\n",
    "        predictions.append(compact_fen_row)\n",
    "\n",
    "    fen_result = '/'.join(predictions)\n",
    "    print(\"Generated FEN:\", fen_result)\n",
    "\n",
    "    # Save FEN to file\n",
    "    with open(\"generated_bw_fen.fen\", \"w\") as file:\n",
    "        file.write(fen_result)\n",
    "\n",
    "    # Save overlay image\n",
    "    cv2.imwrite(output_path, color_image)\n",
    "\n",
    "# Example usage\n",
    "image_path = 'chess_images/test2.jpg'\n",
    "split_and_predict_fen_with_overlay(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raspberryturk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
